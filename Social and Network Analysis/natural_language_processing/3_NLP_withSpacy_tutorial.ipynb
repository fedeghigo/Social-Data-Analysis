{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Spacy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --user spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token analysis\n",
      "\n",
      "text: Mr., is start: True\n",
      " POS: PROPN, LEMMA: Mr.\n",
      "text: Smith, is start: False\n",
      " POS: PROPN, LEMMA: Smith\n",
      "text: made, is start: False\n",
      " POS: VERB, LEMMA: make\n",
      "text: a, is start: False\n",
      " POS: DET, LEMMA: a\n",
      "text: deal, is start: False\n",
      " POS: NOUN, LEMMA: deal\n",
      "text: on, is start: False\n",
      " POS: ADP, LEMMA: on\n",
      "text: a, is start: False\n",
      " POS: DET, LEMMA: a\n",
      "text: beach, is start: False\n",
      " POS: NOUN, LEMMA: beach\n",
      "text: of, is start: False\n",
      " POS: ADP, LEMMA: of\n",
      "text: Switzerland, is start: False\n",
      " POS: PROPN, LEMMA: Switzerland\n",
      "text: near, is start: False\n",
      " POS: ADP, LEMMA: near\n",
      "text: WHO, is start: False\n",
      " POS: PRON, LEMMA: who\n",
      "text: ., is start: False\n",
      " POS: PUNCT, LEMMA: .\n"
     ]
    }
   ],
   "source": [
    "text = \"Mr. Smith made a deal on a beach of Switzerland near WHO.\"\n",
    "\n",
    "doc = nlp(text)  \n",
    "\n",
    "print(\"token analysis\\n\")\n",
    "for token in doc:\n",
    "    print(\"text: \"+token.text+\", is start: \"+str(token.is_sent_start)+\"\\n POS: \"+token.pos_+\", LEMMA: \" + token.lemma_)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr., is a stopword? False\n",
      " alphab. chars: False\n",
      "Smith, is a stopword? False\n",
      " alphab. chars: True\n",
      "made, is a stopword? True\n",
      " alphab. chars: True\n",
      "a, is a stopword? True\n",
      " alphab. chars: True\n",
      "deal, is a stopword? False\n",
      " alphab. chars: True\n",
      "on, is a stopword? True\n",
      " alphab. chars: True\n",
      "a, is a stopword? True\n",
      " alphab. chars: True\n",
      "beach, is a stopword? False\n",
      " alphab. chars: True\n",
      "of, is a stopword? True\n",
      " alphab. chars: True\n",
      "Switzerland, is a stopword? False\n",
      " alphab. chars: True\n",
      "near, is a stopword? False\n",
      " alphab. chars: True\n",
      "WHO, is a stopword? True\n",
      " alphab. chars: True\n",
      "., is a stopword? False\n",
      " alphab. chars: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text+\", is a stopword? \"+str(token.is_stop)+\"\\n alphab. chars: \"+str(token.is_alpha))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Chunks\n",
    "\n",
    "Spacy automatically finds the noun chunks in the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunks(doc):\n",
    "    print(\"chunk analysis\\n\")\n",
    "    num_chunks = 0\n",
    "    for chunk in doc.noun_chunks:\n",
    "        num_chunks = num_chunks + 1\n",
    "        print(\"chunk text: \"+chunk.text+\"\\n root text: \"+chunk.root.text+\", root dep: \"+ chunk.root.dep_+\", root head: \"+chunk.root.head.text+\"\\n\\n\")\n",
    "    num_chunks = len(list(doc.noun_chunks))\n",
    "    print(\"total number of chunks: \"+str(num_chunks))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Smith made a deal on a beach of Switzerland near WHO.\n",
      "\n",
      "\n",
      "chunk analysis\n",
      "\n",
      "chunk text: Mr. Smith\n",
      " root text: Smith, root dep: nsubj, root head: made\n",
      "\n",
      "\n",
      "chunk text: a deal\n",
      " root text: deal, root dep: dobj, root head: made\n",
      "\n",
      "\n",
      "chunk text: a beach\n",
      " root text: beach, root dep: pobj, root head: on\n",
      "\n",
      "\n",
      "chunk text: Switzerland\n",
      " root text: Switzerland, root dep: pobj, root head: of\n",
      "\n",
      "\n",
      "chunk text: WHO\n",
      " root text: WHO, root dep: pobj, root head: near\n",
      "\n",
      "\n",
      "total number of chunks: 5\n"
     ]
    }
   ],
   "source": [
    "text = \"Mr. Smith made a deal on a beach of Switzerland near WHO.\"\n",
    "print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "doc = nlp(text)  \n",
    "\n",
    "print(\"chunk analysis\\n\")\n",
    "num_chunks = 0\n",
    "for chunk in doc.noun_chunks:\n",
    "    num_chunks = num_chunks + 1\n",
    "    print(\"chunk text: \"+chunk.text+\"\\n root text: \"+chunk.root.text+\", root dep: \"+ chunk.root.dep_+\", root head: \"+chunk.root.head.text+\"\\n\\n\")\n",
    "num_chunks = len(list(doc.noun_chunks))\n",
    "print(\"total number of chunks: \"+str(num_chunks))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smith 4 9 PERSON\n",
      "Switzerland 36 47 GPE\n",
      "Tot. num. of recognized named entities: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Mr. Smith made a deal on a beach of Switzerland near WHO.\")  \n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "num_ents = len(doc.ents)\n",
    "print(\"Tot. num. of recognized named entities: \"+str(num_ents)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy misses to recognize the organization WHO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
